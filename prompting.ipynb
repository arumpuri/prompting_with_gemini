{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:04.752700Z","iopub.execute_input":"2025-02-02T03:50:04.752985Z","iopub.status.idle":"2025-02-02T03:50:06.278841Z","shell.execute_reply.started":"2025-02-02T03:50:04.752951Z","shell.execute_reply":"2025-02-02T03:50:06.277100Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:50:06.280012Z","iopub.execute_input":"2025-02-02T03:50:06.280684Z","iopub.status.idle":"2025-02-02T03:50:33.262830Z","shell.execute_reply.started":"2025-02-02T03:50:06.280635Z","shell.execute_reply":"2025-02-02T03:50:33.261531Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:51:28.548392Z","iopub.execute_input":"2025-02-02T03:51:28.548784Z","iopub.status.idle":"2025-02-02T03:51:30.511078Z","shell.execute_reply.started":"2025-02-02T03:51:28.548752Z","shell.execute_reply":"2025-02-02T03:51:30.510052Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:51:33.419668Z","iopub.execute_input":"2025-02-02T03:51:33.420434Z","iopub.status.idle":"2025-02-02T03:51:33.614899Z","shell.execute_reply.started":"2025-02-02T03:51:33.420392Z","shell.execute_reply":"2025-02-02T03:51:33.613825Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain Malware analysis to me like I'm a kid.\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:52:35.974146Z","iopub.execute_input":"2025-02-02T03:52:35.974558Z","iopub.status.idle":"2025-02-02T03:52:38.566716Z","shell.execute_reply.started":"2025-02-02T03:52:35.974524Z","shell.execute_reply":"2025-02-02T03:52:38.565210Z"}},"outputs":[{"name":"stdout","text":"Imagine you found a weird, shiny candy on the ground.  You don't know what's inside!  It *looks* yummy, but maybe it's poisonous or makes you sick.\n\nMalware analysis is like carefully examining that candy before you eat it.  Instead of candy, it's a computer program that might be bad – a virus, a worm, or something else sneaky.\n\nExperts, like computer detectives, use special tools to look inside this program:\n\n* **They open it up very slowly and carefully:** They don't just run it on their own computer, because it could damage it! They use special \"sandboxes\" – like a safe play area – to see what the program does without hurting anything important.\n* **They look at the ingredients:** They check the program's code to see what it's *supposed* to do, and what it's *actually* doing.  Is it stealing your secrets?  Is it trying to slow down your computer? Is it sending messages to bad guys?\n* **They taste a tiny bit (metaphorically!):**  They might run a small part of the program in the sandbox to see how it behaves.\n\n\nAfter all this careful detective work, they can figure out if the program is safe or dangerous.  They can also tell others what to watch out for to stay safe from the \"bad candy\".\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Athrun.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:53:58.090520Z","iopub.execute_input":"2025-02-02T03:53:58.090981Z","iopub.status.idle":"2025-02-02T03:53:58.803865Z","shell.execute_reply.started":"2025-02-02T03:53:58.090947Z","shell.execute_reply":"2025-02-02T03:53:58.802639Z"}},"outputs":[{"name":"stdout","text":"It's nice to meet you, Athrun!  How can I help you today?\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:54:17.442563Z","iopub.execute_input":"2025-02-02T03:54:17.442909Z","iopub.status.idle":"2025-02-02T03:54:18.444457Z","shell.execute_reply.started":"2025-02-02T03:54:17.442885Z","shell.execute_reply":"2025-02-02T03:54:18.443200Z"}},"outputs":[{"name":"stdout","text":"Did you know that some dinosaurs had feathers?  While we often picture dinosaurs as scaly reptiles,  many, especially theropods (the group that includes *Tyrannosaurus rex* and *Velociraptor*), sported feathers, ranging from simple filaments to complex, flight-capable plumage.  This discovery significantly changed our understanding of dinosaur evolution and their relationship to birds, which are now considered to be descended from feathered dinosaurs.\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"response = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:55:24.770668Z","iopub.execute_input":"2025-02-02T03:55:24.771049Z","iopub.status.idle":"2025-02-02T03:55:25.202249Z","shell.execute_reply.started":"2025-02-02T03:55:24.771023Z","shell.execute_reply":"2025-02-02T03:55:25.200988Z"}},"outputs":[{"name":"stdout","text":"Yes, your name is Athrun.\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:55:47.980043Z","iopub.execute_input":"2025-02-02T03:55:47.980437Z","iopub.status.idle":"2025-02-02T03:55:48.335374Z","shell.execute_reply.started":"2025-02-02T03:55:47.980407Z","shell.execute_reply":"2025-02-02T03:55:48.334267Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/gemini-2.0-flash-exp\nmodels/gemini-exp-1206\nmodels/gemini-exp-1121\nmodels/gemini-exp-1114\nmodels/gemini-2.0-flash-thinking-exp-01-21\nmodels/gemini-2.0-flash-thinking-exp\nmodels/gemini-2.0-flash-thinking-exp-1219\nmodels/learnlm-1.5-pro-experimental\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:56:16.084391Z","iopub.execute_input":"2025-02-02T03:56:16.084877Z","iopub.status.idle":"2025-02-02T03:56:16.381939Z","shell.execute_reply.started":"2025-02-02T03:56:16.084843Z","shell.execute_reply":"2025-02-02T03:56:16.380628Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# When generating text with an LLM, the output length affects cost \n# and performance. Generating more tokens increases computation,\n# leading to higher energy consumption, latency, and cost.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nshort_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:58:08.646646Z","iopub.execute_input":"2025-02-02T03:58:08.647124Z","iopub.status.idle":"2025-02-02T03:58:10.539172Z","shell.execute_reply.started":"2025-02-02T03:58:08.647087Z","shell.execute_reply":"2025-02-02T03:58:10.537663Z"}},"outputs":[{"name":"stdout","text":"## The Enduring Significance of Olives in Modern Society\n\nThe olive, a seemingly unassuming fruit, holds a position of remarkable significance in modern society, extending far beyond its culinary applications.  Its impact resonates through history, culture, economy, and even environmental sustainability, shaping landscapes, livelihoods, and global markets in profound ways. From the ancient Mediterranean civilizations that revered it to contemporary debates surrounding sustainable agriculture and healthy diets, the olive continues to hold a central place in the human story.\n\nThe olive's primary contribution to modern society undeniably rests on its culinary role.  Olive oil, extracted from the fruit, is a staple ingredient in cuisines worldwide, prized for its unique flavor profile, nutritional benefits, and versatility.  Beyond its use as a cooking oil, olive oil serves as a base for countless dressings, marinades, and sauces. Its incorporation into diverse culinary traditions, from the Mediterranean diet to contemporary fusion cuisine, testifies to its enduring appeal and adaptability.  The characteristic flavor of olive oil,\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T03:59:12.454742Z","iopub.execute_input":"2025-02-02T03:59:12.455106Z","iopub.status.idle":"2025-02-02T03:59:13.357173Z","shell.execute_reply.started":"2025-02-02T03:59:12.455081Z","shell.execute_reply":"2025-02-02T03:59:13.355404Z"}},"outputs":[{"name":"stdout","text":"From ancient groves, a modern grace,\nThe olive's gift, in time and place.\nIn oil it shines, a kitchen star,\nOn salads dressed, both near and far.\n\nA taste of sun, a healthy gleam,\nIn beauty products, a vibrant dream.\nFrom humble fruit, a bounty grows,\nThe olive thrives, where richness flows. \n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Temperature controls the degree of randomness in token selection.\n# Higher temperatures result in a higher number of candidate tokens \n# from which the next output token is selected","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:02:12.171362Z","iopub.execute_input":"2025-02-02T04:02:12.171845Z","iopub.status.idle":"2025-02-02T04:02:14.051305Z","shell.execute_reply.started":"2025-02-02T04:02:12.171811Z","shell.execute_reply":"2025-02-02T04:02:14.049934Z"}},"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nAquamarine\n -------------------------\nMaroon\n -------------------------\nPurple\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:03:23.993088Z","iopub.execute_input":"2025-02-02T04:03:23.993548Z","iopub.status.idle":"2025-02-02T04:03:26.057443Z","shell.execute_reply.started":"2025-02-02T04:03:23.993513Z","shell.execute_reply":"2025-02-02T04:03:26.056138Z"}},"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Top-K and top-P","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top-K is a positive integer that defines the number of most probable \n# tokens from which to select the output token. A top-K of 1 selects a \n# single token, performing greedy decoding.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top-P defines the probability threshold that, once cumulatively \n# exceeded, tokens stop being selected as candidates. A top-P of 0 is \n# typically equivalent to greedy decoding, and a top-P of 1 typically \n# selects every token in the model's vocabulary.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:06:35.769446Z","iopub.execute_input":"2025-02-02T04:06:35.769880Z","iopub.status.idle":"2025-02-02T04:06:38.891027Z","shell.execute_reply.started":"2025-02-02T04:06:35.769850Z","shell.execute_reply":"2025-02-02T04:06:38.889171Z"}},"outputs":[{"name":"stdout","text":"Bartholomew, a ginger tabby with a penchant for mischief and a disdain for routine, found himself staring at the window with an air of profound discontent. The sun, a fiery ball of orange, was setting, casting long shadows across the garden, and Bartholomew was bored. He'd chased butterflies, pestered the goldfish, and meticulously inspected every inch of the living room rug - all in the span of a single afternoon. Adventure, he craved adventure! \n\nThe garden gate, left ajar by a forgetful human, beckoned. With a flick of his tail, Bartholomew slipped out, his whiskers twitching with anticipation. The world beyond the garden, with its rustling leaves and chirping crickets, was a symphony of possibilities.\n\nHe followed the scent of freshly mown grass, his paws leaving tiny tracks in the dew-laden earth. He stalked a plump robin, its red breast a tempting target, before deciding it was too much trouble for a mere snack. He scaled a tall oak, the bark rough beneath his claws, and surveyed his domain - a sprawling meadow bathed in the golden light of the setting sun.\n\nA rustle in the undergrowth caught his attention. A tiny, grey creature with big, dark eyes stared back at him. Bartholomew, in a moment of pure, feline instinct, pounced. It was a mouse, its tiny heart thumping in its chest. But instead of devouring it, Bartholomew found himself charmed by its fear.\n\nThe mouse, gathering its courage, squeaked, \"Don't eat me, please! I'm lost.\" Bartholomew, a cat of noble spirit, felt a pang of sympathy. \"Lost? I know just the place for lost things,\" he purred, leading the mouse back to the garden.\n\nBack home, Bartholomew's human, a young girl named Lily, was frantic. The garden gate stood open, and Bartholomew was nowhere to be found. When he returned, with a tiny, trembling mouse tucked under his paw, Lily was overjoyed. \n\nBartholomew, as he curled up beside Lily on the couch, felt a warmth spread through him. Adventure was all well and good, but there was nothing quite like a warm lap and the gentle strokes of a human hand. He had found his adventure, and a new friend, all in one evening. And as he drifted off to sleep, a contented purr vibrated through his chest, a testament to a day well spent. \n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Prompting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zero-shot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:08:25.273870Z","iopub.execute_input":"2025-02-02T04:08:25.274347Z","iopub.status.idle":"2025-02-02T04:08:25.796897Z","shell.execute_reply.started":"2025-02-02T04:08:25.274311Z","shell.execute_reply":"2025-02-02T04:08:25.795445Z"}},"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Enum mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:10:57.991705Z","iopub.execute_input":"2025-02-02T04:10:57.992120Z","iopub.status.idle":"2025-02-02T04:10:58.817530Z","shell.execute_reply.started":"2025-02-02T04:10:57.992089Z","shell.execute_reply":"2025-02-02T04:10:58.816696Z"}},"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# One-shot and few-shot","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:12:39.900175Z","iopub.execute_input":"2025-02-02T04:12:39.900591Z","iopub.status.idle":"2025-02-02T04:12:40.487252Z","shell.execute_reply.started":"2025-02-02T04:12:39.900560Z","shell.execute_reply":"2025-02-02T04:12:40.485978Z"}},"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# JSON mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:13:34.331656Z","iopub.execute_input":"2025-02-02T04:13:34.332080Z","iopub.status.idle":"2025-02-02T04:13:35.127252Z","shell.execute_reply.started":"2025-02-02T04:13:34.332050Z","shell.execute_reply":"2025-02-02T04:13:35.126040Z"}},"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Chain of Thought (CoT)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:15:46.452595Z","iopub.execute_input":"2025-02-02T04:15:46.453066Z","iopub.status.idle":"2025-02-02T04:15:46.824093Z","shell.execute_reply.started":"2025-02-02T04:15:46.453033Z","shell.execute_reply":"2025-02-02T04:15:46.822886Z"}},"outputs":[{"name":"stdout","text":"41\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:16:05.596451Z","iopub.execute_input":"2025-02-02T04:16:05.596972Z","iopub.status.idle":"2025-02-02T04:16:06.744460Z","shell.execute_reply.started":"2025-02-02T04:16:05.596923Z","shell.execute_reply":"2025-02-02T04:16:06.743245Z"}},"outputs":[{"name":"stdout","text":"1. **Partner's age when you were 4:** When you were 4, your partner was 3 times your age, meaning they were 3 * 4 = 12 years old.\n\n2. **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3. **Partner's current age:** Since you are now 20 years old, and the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ReAct: Reason and act","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:18:45.334129Z","iopub.execute_input":"2025-02-02T04:18:45.334557Z","iopub.status.idle":"2025-02-02T04:18:45.340712Z","shell.execute_reply.started":"2025-02-02T04:18:45.334524Z","shell.execute_reply":"2025-02-02T04:18:45.339445Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:18:54.163405Z","iopub.execute_input":"2025-02-02T04:18:54.163902Z","iopub.status.idle":"2025-02-02T04:18:56.709079Z","shell.execute_reply.started":"2025-02-02T04:18:54.163864Z","shell.execute_reply":"2025-02-02T04:18:56.707786Z"}},"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers NLP paper and then find the authors and their ages.  This will require multiple searches and likely some outside knowledge I don't have access to here.\n\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:19:37.505114Z","iopub.execute_input":"2025-02-02T04:19:37.505628Z","iopub.status.idle":"2025-02-02T04:19:38.647414Z","shell.execute_reply.started":"2025-02-02T04:19:37.505574Z","shell.execute_reply":"2025-02-02T04:19:38.645268Z"}},"outputs":[{"name":"stdout","text":"Thought 2\nThe observation gives me the authors of the paper \"Attention is All You Need,\" but not their ages. I cannot directly answer the question using this information. I need to find another way to determine their ages.  This will likely require external resources, which are not accessible within this framework.\n\nAction 2\n<finish>I cannot answer this question.  The provided search only returns the authors of the paper, not their ages at the time of publication.</finish>\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Code prompting","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:21:35.129745Z","iopub.execute_input":"2025-02-02T04:21:35.130216Z","iopub.status.idle":"2025-02-02T04:21:35.790422Z","shell.execute_reply.started":"2025-02-02T04:21:35.130180Z","shell.execute_reply":"2025-02-02T04:21:35.789074Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)\n```\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# Code execution","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:23:21.236381Z","iopub.execute_input":"2025-02-02T04:23:21.236852Z","iopub.status.idle":"2025-02-02T04:23:26.123985Z","shell.execute_reply.started":"2025-02-02T04:23:21.236816Z","shell.execute_reply":"2025-02-02T04:23:26.122899Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to generate these numbers and then compute their sum.\n\n\n``` python\nimport sympy\n\ndef is_prime(n):\n  \"\"\"Checks if a number is prime using sympy.\"\"\"\n  return sympy.isprime(n)\n\ncount = 0\nsum_primes = 0\nnum = 1\nwhile count < 14:\n    num += 2 #Start from 3, and only consider odd numbers\n    if is_prime(num):\n        sum_primes += num\n        count += 1\n\nprint(f\"The sum of the first 14 odd prime numbers is: {sum_primes}\")\n\n\n```\n```\nThe sum of the first 14 odd prime numbers is: 326\n\n```\nTherefore, the sum of the first 14 odd prime numbers is 326.\n"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:23:48.547056Z","iopub.execute_input":"2025-02-02T04:23:48.547463Z","iopub.status.idle":"2025-02-02T04:23:48.558230Z","shell.execute_reply.started":"2025-02-02T04:23:48.547433Z","shell.execute_reply":"2025-02-02T04:23:48.555887Z"}},"outputs":[{"name":"stdout","text":"text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  I will use Python to generate these numbers and then compute their sum.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\nimport sympy\\n\\ndef is_prime(n):\\n  \\\"\\\"\\\"Checks if a number is prime using sympy.\\\"\\\"\\\"\\n  return sympy.isprime(n)\\n\\ncount = 0\\nsum_primes = 0\\nnum = 1\\nwhile count < 14:\\n    num += 2 #Start from 3, and only consider odd numbers\\n    if is_prime(num):\\n        sum_primes += num\\n        count += 1\\n\\nprint(f\\\"The sum of the first 14 odd prime numbers is: {sum_primes}\\\")\\n\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"The sum of the first 14 odd prime numbers is: 326\\n\"\n}\n\n-----\ntext: \"Therefore, the sum of the first 14 odd prime numbers is 326.\\n\"\n\n-----\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T04:24:49.115159Z","iopub.execute_input":"2025-02-02T04:24:49.115597Z","iopub.status.idle":"2025-02-02T04:24:52.377960Z","shell.execute_reply.started":"2025-02-02T04:24:49.115564Z","shell.execute_reply":"2025-02-02T04:24:52.376641Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a bash script (a shell script for bash or zsh) that provides a highly customizable Git prompt for your terminal.  In simpler terms, it makes your command prompt show information about your current Git repository, such as the branch, status (changes, conflicts, etc.), and optionally, even the remote repository URL.\n\nYou would use this script to enhance your command-line experience when working with Git. Instead of just seeing a plain prompt, you'll get a prompt that dynamically updates to reflect the state of your Git repository. This helps you quickly understand the current branch, if there are uncommitted changes, or other relevant Git status information without having to explicitly run `git status` every time.\n\nThe script offers features like:\n\n* **Theming:** Customizable color schemes.\n* **Information Display:**  Shows branch name, changes, commits ahead/behind remote, and more.\n* **Remote Status:** Periodically fetches remote updates to show if your local branch is ahead or behind.\n* **Configuration:** You can configure which information is shown and how it's displayed via variables in your `.bashrc` or `.zshrc`.\n* **Compatibility:** Works with both bash and zsh.\n\nEssentially, it's a power-up for your Git workflow by embedding Git repository information directly into your shell prompt, giving you more context at a glance.\n"},"metadata":{}}],"execution_count":29}]}